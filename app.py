# app.py - MULTI-PAGE RAG CHATBOT (FULL & T·ªêI ∆ØU)
import os
import json
import time
import random
import requests
import threading
from pathlib import Path
# C·∫ßn import numpy v√† tqdm ƒë·ªÉ ch·∫°y c√°c h√†m t√≠nh to√°n v√† hi·ªÉn th·ªã ti·∫øn tr√¨nh
import numpy as np
from tqdm import tqdm 
from flask import Flask, request, jsonify
from openai import OpenAI

# -----------------------
# CONFIG
# -----------------------
EMBED_MODEL = "text-embedding-3-large"
CHAT_MODEL = "gpt-4o-mini"
EMBED_BATCH = 16
CHUNK_SIZE = 400
SIMILARITY_THRESHOLD = 0.72
TOP_K = 5
TEMPERATURE = 0.12

PAGE_TOKEN = os.environ.get("PAGE_ACCESS_TOKEN", "")
VERIFY_TOKEN = os.environ.get("VERIFY_TOKEN", "")
OPENAI_KEY = os.environ.get("OPENAI_API_KEY", "")
FB_SEND_URL = f"https://graph.facebook.com/v19.0/me/messages?access_token={PAGE_TOKEN}"

# -----------------------
# PAGE ‚Üí DATA FOLDER MAPPING
# -----------------------
PAGE_DATASET_MAP = {
    "895305580330861": "page_xyz",     # Ki·∫øn tr√∫c XYZ
    "847842948414951": "page_ctt"      # Chatbot CTT
}

DATA_FOLDER_ROOT = Path("data")
EMBEDDINGS_FOLDER = Path("embeddings")

app = Flask(__name__)

# -----------------------
# OpenAI
# -----------------------
try:
    client = OpenAI(api_key=OPENAI_KEY)
    print("‚úÖ OpenAI client ready")
except Exception as e:
    print("‚ùå OpenAI init error:", e)
    client = None

# -----------------------
# STORAGE
# -----------------------
# DATABASE[folder_name] = { file_key: content_dict, ... }
DATABASE = {}
# CORPUS[folder_name] = [ chunk_dict, ... ]
CORPUS = {}
# EMBEDDINGS[folder_name] = { "vectors": [...], "meta": {...} }
EMBEDDINGS = {}

# -----------------------
# LOAD DATASET
# -----------------------
def load_dataset_by_folder(folder):
    folder_path = DATA_FOLDER_ROOT / folder
    db = {}
    if not folder_path.exists():
        print(f"‚ùå data/{folder} folder missing")
        return db
    for f in folder_path.glob("*.json"):
        try:
            with open(f, "r", encoding="utf8") as fh:
                db[f.stem] = json.load(fh)
        except Exception as e:
            print(f"‚ùå Load fail {f}: {e}")
            pass
    print(f"üìÇ Loaded data for '{folder}': {list(db.keys())}")
    return db

# -----------------------
# CHUNKING + CORPUS
# -----------------------
def text_to_chunks(text, size=CHUNK_SIZE):
    text = text.strip().replace("\n", " ") # D√πng "\n" thay cho kho·∫£ng tr·∫Øng
    if not text:
        return []
    
    parts = text.split(". ")
    chunks = []
    cur = ""
    for p in parts:
        if len(cur) + len(p) + 2 <= size:
            cur = (cur + ". " + p).strip(" .")
        else:
            if cur:
                chunks.append(cur)
            cur = p
    if cur:
        chunks.append(cur)

    final = []
    for c in chunks:
        if len(c) <= size:
            final.append(c)
        else:
            for i in range(0, len(c), size):
                final.append(c[i:i+size])
    return final

def build_corpus_from_database(db):
    corpus = []
    idx = 0
    for file_key, content in db.items():
        for tr in content.get("chatbot_triggers", []):
            kw = " ".join(tr.get("keywords", []))
            resp = tr.get("response", "")
            if isinstance(resp, list): resp = " ".join(resp)
            # D√πng \n thay cho kho·∫£ng tr·∫Øng
            text = f"KEYWORDS: {kw}\nRESPONSE: {resp}"
            for c in text_to_chunks(text):
                corpus.append({"id": f"c{idx}", "file": file_key, "source": "trigger", "text": c})
                idx += 1

        for p in content.get("products", []):
            name = p.get("name", "")
            desc = p.get("description", "")
            if not isinstance(desc, str): desc = json.dumps(desc, ensure_ascii=False)
            text = f"PRODUCT: {name}\n{desc}"
            for c in text_to_chunks(text):
                corpus.append({"id": f"c{idx}", "file": file_key, "source": "product", "text": c})
                idx += 1

        for pr in content.get("highlight_projects", []):
            name = pr.get("name", "")
            desc = pr.get("summary", "")
            if not isinstance(desc, str): desc = json.dumps(desc, ensure_ascii=False)
            text = f"PROJECT: {name}\n{desc}"
            for c in text_to_chunks(text):
                corpus.append({"id": f"c{idx}", "file": file_key, "source": "project", "text": c})
                idx += 1

        persona = content.get("persona", {})
        if persona:
            text = f"PERSONA: {persona.get('role','')}. {persona.get('tone','')}. Goal: {persona.get('goal','')}"
            for c in text_to_chunks(text):
                corpus.append({"id": f"c{idx}", "file": file_key, "source": "persona", "text": c})
                idx += 1
    return corpus

# -----------------------
# EMBEDDING
# -----------------------
def get_embed_path(folder):
    EMBEDDINGS_FOLDER.mkdir(exist_ok=True) # ƒê·∫£m b·∫£o th∆∞ m·ª•c embeddings t·ªìn t·∫°i
    return EMBEDDINGS_FOLDER / f"{folder}.json"

def compute_embeddings_for_page(folder, corpus, force=False):
    embed_path = get_embed_path(folder)

    if embed_path.exists() and not force:
        try:
            with open(embed_path, "r", encoding="utf8") as fh:
                existing = json.load(fh)
                if len(existing.get("vectors", [])) == len(corpus):
                    print(f"üóÑÔ∏è Load embeddings for '{folder}' from disk.")
                    return existing
                else:
                    print(f"‚ö†Ô∏è Embedding count mismatch for '{folder}'. Rebuilding.")
        except Exception as e:
            print(f"‚ùå Load embedding error for '{folder}': {e}. Rebuilding.")
            pass # Chuy·ªÉn sang build m·ªõi

    print(f"‚öôÔ∏è Creating embeddings for '{folder}' ({len(corpus)} chunks)...")

    vectors = []
    texts = [c["text"] for c in corpus]
    batch, batch_idx = [], []

    for i, txt in enumerate(tqdm(texts, desc=f"Embedding {folder}")): # D√πng tqdm ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh
        batch.append(txt)
        batch_idx.append(i)

        if len(batch) >= EMBED_BATCH or i == len(texts) - 1:
            try:
                resp = client.embeddings.create(model=EMBED_MODEL, input=batch)
                for j, out in enumerate(resp.data):
                    idx = batch_idx[j]
                    c = corpus[idx]
                    vectors.append({
                        "id": c["id"], "file": c["file"], "source": c["source"],
                        "text": c["text"], "vec": out.embedding
                    })
            except Exception as e:
                print(f"‚ùå Embedding API error (batch {i//EMBED_BATCH}):", e)
                for j, _ in enumerate(batch):
                    idx = batch_idx[j]
                    c = corpus[idx]
                    vectors.append({
                        "id": c["id"], "file": c["file"], "source": c["source"],
                        "text": c["text"], "vec": [0.0]*1536 # K√≠ch th∆∞·ªõc 1536 cho text-embedding-3-large
                    })
            batch, batch_idx = [], []

    store = {"vectors": vectors, "meta": {"created": time.time()}}
    with open(embed_path, "w", encoding="utf8") as fh:
        json.dump(store, fh, ensure_ascii=False, indent=2)
    print(f"‚úÖ Embeddings saved: {embed_path}")
    return store

# -----------------------
# SIMILARITY
# -----------------------
def cosine(a, b):
    # ƒê·∫£m b·∫£o a, b l√† numpy array
    a = np.array(a, dtype=float) 
    b = np.array(b, dtype=float)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    if norm_a == 0 or norm_b == 0: return 0.0
    return float(np.dot(a, b) / (norm_a * norm_b))

def semantic_search(query_vec, store, top_k=TOP_K):
    sims = []
    for x in store["vectors"]:
        sims.append({"score": cosine(query_vec, x["vec"]), "item": x})
    sims.sort(key=lambda x: x["score"], reverse=True)
    return sims[:top_k]

# -----------------------
# SEMANTIC CONTEXT
# -----------------------
def get_semantic_context(folder, text):
    store = EMBEDDINGS.get(folder, {})
    if not store or not store.get("vectors"):
        print(f"‚ö†Ô∏è Embeddings not ready for {folder}")
        return []

    try:
        resp = client.embeddings.create(model=EMBED_MODEL, input=[text])
        qvec = resp.data[0].embedding
    except Exception as e:
        print("‚ùå Query embedding error:", e)
        return []
    
    return semantic_search(qvec, store)

# -----------------------
# FAST JSON MATCH
# -----------------------
def find_in_json_exact(folder, text):
    db = DATABASE.get(folder, {})
    t = text.lower()
    for file_key, data in db.items():
        for tr in data.get("chatbot_triggers", []):
            for k in tr.get("keywords", []):
                # Ki·ªÉm tra match token nghi√™m ng·∫∑t h∆°n
                k_lower = k.lower()
                if k_lower and (f" {k_lower} " in f" {t} " or t.startswith(k_lower + " ") or t.endswith(" " + k_lower) or t == k_lower):
                    resp = tr.get("response", "")
                    if isinstance(resp, list): return random.choice(resp)
                    return random.choice(resp.splitlines()) # Ch·ªçn ng·∫´u nhi√™n 1 d√≤ng n·∫øu c√≥ nhi·ªÅu d√≤ng
    return None

# -----------------------
# SYSTEM PROMPT
# -----------------------
def assemble_system_prompt(folder, user_text, top_items):
    files = [i["item"]["file"] for i in top_items]
    dominant = max(set(files), key=files.count)
    persona = {}

    # L·∫•y persona t·ª´ file dominant
    for file_key, data in DATABASE.get(folder, {}).items():
        if file_key == dominant:
            persona = data.get("persona", {})
            break

    ctx = []
    for x in top_items:
        it = x["item"]
        ctx.append(f"[file:{it['file']} score:{x['score']:.3f}]\n{it['text']}")

    ctx_text = "\n\n---\n\n".join(ctx)

    return f"""
B·∫°n l√† tr·ª£ l√Ω h·ªó tr·ª£ kh√°ch h√†ng, tr·∫£ l·ªùi d·ª±a tr√™n ƒë√∫ng d·ªØ li·ªáu cung c·∫•p.
Persona: {json.dumps(persona, ensure_ascii=False)}

--- QUY T·∫ÆC R·∫§T CH·∫∂T ---
1) Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n ph·∫ßn CONTEXT d∆∞·ªõi ƒë√¢y. Kh√¥ng th√™m, kh√¥ng suy di·ªÖn.
2) N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng th·ªÉ r√∫t ra t·ª´ CONTEXT ‚Üí Tr·∫£ l·ªùi: "M√¨nh ch∆∞a c√≥ th√¥ng tin c·ª• th·ªÉ, b·∫°n cho m√¨nh bi·∫øt r√µ h∆°n ƒë∆∞·ª£c kh√¥ng?"
3) Tr·∫£ l·ªùi ng·∫Øn g·ªçn 1-3 c√¢u, tr·ª±c ti·∫øp.

--- USER:
"{user_text}"

--- CONTEXT (Ch·ªâ d√πng ph·∫ßn n√†y):
{ctx_text}
"""

# -----------------------
# CALL OPENAI
# -----------------------
def ask_llm(system_prompt, user_text):
    try:
        resp = client.chat.completions.create(
            model=CHAT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_text}
            ],
            temperature=TEMPERATURE,
            max_tokens=250
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print("‚ùå OpenAI chat error:", e)
        return "H·ªá th·ªëng AI ƒëang b·∫≠n, b·∫°n th·ª≠ l·∫°i sau 1 ph√∫t nh√©."

# -----------------------
# SMART REPLY
# -----------------------
def ensure_page_data(folder, force=False):
    """ƒê·∫£m b·∫£o data, corpus v√† embeddings ƒë√£ ƒë∆∞·ª£c t·∫£i/t·∫°o cho folder."""
    if folder not in DATABASE or force:
        DATABASE[folder] = load_dataset_by_folder(folder)
        CORPUS[folder] = build_corpus_from_database(DATABASE[folder])
        EMBEDDINGS[folder] = compute_embeddings_for_page(folder, CORPUS[folder], force=force)

def get_smart_reply(folder, text):
    # 1) exact match
    fast = find_in_json_exact(folder, text)
    if fast:
        return fast

    # 2) ensure dataset loaded (ch·∫°y n·ªÅn l·∫ßn ƒë·∫ßu)
    ensure_page_data(folder)

    # 3) semantic search
    sims = get_semantic_context(folder, text)
    if not sims:
        return "B·∫°n mu·ªën h·ªèi v·ªÅ d·ªãch v·ª• n√†o ƒë·ªÉ m√¨nh h·ªó tr·ª£ r√µ h∆°n?"

    # 4) score check
    if sims[0]["score"] < SIMILARITY_THRESHOLD:
        return "M√¨nh ch∆∞a r√µ b·∫°n h·ªèi v·ªÅ n·ªôi dung n√†o - b·∫°n m√¥ t·∫£ c·ª• th·ªÉ h∆°n gi√∫p m√¨nh nh√©."

    # 5) filter by dominant file
    files = [s["item"]["file"] for s in sims]
    dominant = max(set(files), key=files.count)
    top_items = [s for s in sims if s["item"]["file"] == dominant]
    if not top_items:
        top_items = sims # Fallback n·∫øu dominant file kh√¥ng c√≥ items

    # 6) prompt + llm
    prompt = assemble_system_prompt(folder, text, top_items)
    return ask_llm(prompt, text)

# -----------------------
# FACEBOOK SEND
# -----------------------
def send_text(psid, text):
    try:
        requests.post(FB_SEND_URL, json={
            "recipient": {"id": psid},
            "message": {"text": text}
        }, timeout=15)
    except Exception as e:
        print("‚ùå FB send error:", e)

# -----------------------
# WEBHOOK
# -----------------------
@app.route("/webhook", methods=["GET"])
def verify():
    if request.args.get("hub.verify_token") == VERIFY_TOKEN:
        return request.args.get("hub.challenge")
    return "Sai verify token", 403

@app.route("/webhook", methods=["POST"])
def webhook():
    data = request.get_json(silent=True) or {}
    for entry in data.get("entry", []):
        page_id = str(entry.get("id"))
        folder = PAGE_DATASET_MAP.get(page_id)

        if not folder:
            print(f"‚ö†Ô∏è Page ID {page_id} ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. B·ªè qua.")
            continue

        for evt in entry.get("messaging", []):
            if evt.get("message", {}).get("is_echo"):
                continue
            psid = evt.get("sender", {}).get("id")
            text = evt.get("message", {}).get("text")
            
            if psid and text:
                print(f"üåê Page:{folder} | üë§ {psid} -> {text}")
                reply = get_smart_reply(folder, text)
                print(f"ü§ñ reply ({folder}):", reply)
                # D√πng threading ƒë·ªÉ g·ª≠i tin nh·∫Øn kh√¥ng block lu·ªìng x·ª≠ l√Ω webhook
                threading.Thread(target=send_text, args=(psid, reply)).start()
                
    return "OK", 200

# -----------------------
# HEALTH
# -----------------------
@app.route("/health")
def health():
    status = {}
    for page_id, folder_name in PAGE_DATASET_MAP.items():
        embed_count = len(EMBEDDINGS.get(folder_name, {}).get("vectors", []))
        data_files = list(DATABASE.get(folder_name, {}).keys())
        status[page_id] = {
            "folder": folder_name,
            "data_files": data_files,
            "embed_count": embed_count,
            "ready": embed_count > 0
        }
    return jsonify(ok=True, pages=status)

# -----------------------
# START SERVER (Kh·ªüi ƒë·ªông Build n·ªÅn)
# -----------------------
def initial_background_build():
    """Kh·ªüi ƒë·ªông qu√° tr√¨nh t·∫£i data v√† build embeddings n·ªÅn cho t·∫•t c·∫£ Pages."""
    print("üöÄ Kh·ªüi ƒë·ªông qu√° tr√¨nh build embeddings n·ªÅn cho t·∫•t c·∫£ Pages...")
    for folder_name in set(PAGE_DATASET_MAP.values()):
        # T·∫£i/Build n·ªÅn, kh√¥ng force
        threading.Thread(
            target=lambda fn=folder_name: ensure_page_data(fn, force=False),
            daemon=True
        ).start()

if __name__ == "__main__":
    initial_background_build()
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 8080)))
