Chuyá»ƒn Ä‘áº¿n ná»™i dung
Lá»‹ch sá»­ Ä‘oáº¡n chat

Báº¡n Ä‘Ã£ nÃ³i:
# app.py
import os
import json
import math
import time
import random
import requests
import threading
from pathlib import Path
from tqdm import tqdm
import numpy as np
from flask import Flask, request, jsonify
from openai import OpenAI

# -----------------------
#  CONFIG
# -----------------------
EMBED_MODEL = "text-embedding-3-large"
CHAT_MODEL = "gpt-4o-mini"
EMBED_BATCH = 16  # náº¿u cÃ³ nhiá»u text, chia batch
EMBED_FILE = "embeddings_store.json"
CHUNK_SIZE = 400  # kÃ½ tá»± trÃªn chunk (tÃ¹y chá»‰nh)
SIMILARITY_THRESHOLD = 0.72  # náº¿u score tháº¥p hÆ¡n -> há»i láº¡i
TOP_K = 5
TEMPERATURE = 0.12

PAGE_TOKEN = os.environ.get("PAGE_ACCESS_TOKEN", "")
VERIFY_TOKEN = os.environ.get("VERIFY_TOKEN", "")
OPENAI_KEY = os.environ.get("OPENAI_API_KEY", "")

FB_SEND_URL = f"https://graph.facebook.com/v19.0/me/messages?access_token={PAGE_TOKEN}"

app = Flask(__name__)

# -----------------------
#  OpenAI client
# -----------------------
try:
    client = OpenAI(api_key=OPENAI_KEY)
    print("âœ… OpenAI client ready")
except Exception as e:
    print("âŒ OpenAI init error:", e)
    client = None

# -----------------------
#  Load JSON dataset
# -----------------------
DATA_FOLDER = Path("data")
FILE_PRIORITY_ORDER = [
    "quangcao_chatbot_ctt",
    "kientruc_xyz",
    "oc_ngon_18"
]

def load_all_data(folder=DATA_FOLDER):
    db = {}
    if not folder.exists():
        print("âŒ data/ folder missing")
        return db
    for f in folder.glob("*.json"):
        try:
            key = f.stem
            with open(f, "r", encoding="utf8") as fh:
                db[key] = json.load(fh)
        except Exception as e:
            print("âŒ load fail", f, e)
    print("ğŸ“‚ Loaded:", list(db.keys()))
    return db

DATABASE = load_all_data()

# -----------------------
#  Chunking & Indexing
# -----------------------
def text_to_chunks(text, size=CHUNK_SIZE):
    text = text.strip()
    if not text:
        return []
    # chia theo cÃ¢u gáº§n Ä‘Ãºng rá»“i ghÃ©p cho Ä‘á»§ chunk
    parts = text.replace("\n", " ").split(". ")
    chunks = []
    cur = ""
    for p in parts:
        if len(cur) + len(p) + 2 <= size:
            cur = (cur + ". " + p).strip(" .")
        else:
            if cur:
                chunks.append(cur.strip())
            cur = p
    if cur:
        chunks.append(cur.strip())
    # náº¿u váº«n cÃ³ chunk quÃ¡ dÃ i thÃ¬ cáº¯t trá»±c tiáº¿p
    final = []
    for c in chunks:
        if len(c) <= size:
            final.append(c)
        else:
            for i in range(0, len(c), size):
                final.append(c[i:i+size])
    return final

def build_corpus_from_database(db):
    """
    Táº¡o danh sÃ¡ch chunk dict:
    { "id": str, "file": file_key, "source": "trigger|product|project|persona",
      "text": chunk_text }
    """
    corpus = []
    idx = 0
    for file_key, content in db.items():
        # triggers: thÆ°á»ng chá»©a responses and keywords
        for tr in content.get("chatbot_triggers", []):
            # include keywords + response text
            keywords = " ".join(tr.get("keywords", []))
            resp = tr.get("response", "")
            if isinstance(resp, list):
                resp = " ".join(resp)
            text = f"KEYWORDS: {keywords}\nRESPONSE: {resp}"
            for chunk in text_to_chunks(text):
                corpus.append({
                    "id": f"c_{idx}",
                    "file": file_key,
                    "source": "trigger",
                    "text": chunk
                })
                idx += 1

        # products
        for p in content.get("products", []):
            name = p.get("name", "")
            desc = p.get("description", "") if isinstance(p.get("description", ""), str) else json.dumps(p.get("description", ""))
            text = f"PRODUCT: {name}\n{desc}"
            for chunk in text_to_chunks(text):
                corpus.append({
                    "id": f"c_{idx}",
                    "file": file_key,
                    "source": "product",
                    "text": chunk
                })
                idx += 1

        # projects
        for pr in content.get("highlight_projects", []):
            name = pr.get("name", "")
            desc = pr.get("summary", "") if isinstance(pr.get("summary", ""), str) else json.dumps(pr.get("summary", ""))
            text = f"PROJECT: {name}\n{desc}"
            for chunk in text_to_chunks(text):
                corpus.append({
                    "id": f"c_{idx}",
                    "file": file_key,
                    "source": "project",
                    "text": chunk
                })
                idx += 1

        # persona (short)
        persona = content.get("persona", {})
        if persona:
            text = f"PERSONA: {persona.get('role','')}. {persona.get('tone','')}. Goal: {persona.get('goal','')}"
            for chunk in text_to_chunks(text):
                corpus.append({
                    "id": f"c_{idx}",
                    "file": file_key,
                    "source": "persona",
                    "text": chunk
                })
                idx += 1

    return corpus

# -----------------------
#  Embedding store (disk)
# -----------------------
def load_embeddings(path=EMBED_FILE):
    if not Path(path).exists():
        return None
    with open(path, "r", encoding="utf8") as fh:
        return json.load(fh)

def save_embeddings(store, path=EMBED_FILE):
    with open(path, "w", encoding="utf8") as fh:
        json.dump(store, fh, ensure_ascii=False, indent=2)

def compute_embeddings_for_corpus(corpus, force_rebuild=False):
    """
    corpus: list of chunk dicts
    returns store: {"vectors": [{"id":..., "vec":[...], "file":..., "source":..., "text":...}], "meta": {...}}
    """
    existing = load_embeddings()
    if existing and not force_rebuild:
        # quick sanity: if size matches corpus length use it
        if len(existing.get("vectors", [])) == len(corpus):
            print("ğŸ—„ï¸ Load embeddings from disk")
            return existing
        else:
            print("âš ï¸ Embedding count mismatch. Rebuilding.")

    print("âš™ï¸ Creating embeddings for corpus (this may take a while)...")
    vectors = []
    texts = [c["text"] for c in corpus]
    batch = []
    batch_idx = []
    for i, txt in enumerate(tqdm(texts, desc="chunks")):
        batch.append(txt)
        batch_idx.append(i)
        if len(batch) >= EMBED_BATCH or i == len(texts) - 1:
            # call embeddings
            try:
                resp = client.embeddings.create(model=EMBED_MODEL, input=batch)
                # SDK response assumed resp.data[*].embedding
                for j, out in enumerate(resp.data):
                    emb = out.embedding
                    idx = batch_idx[j]
                    c = corpus[idx]
                    vectors.append({
                        "id": c["id"],
                        "file": c["file"],
                        "source": c["source"],
                        "text": c["text"],
                        "vec": emb
                    })
            except Exception as e:
                print("âŒ Embedding API error:", e)
                # fallback: zero vector (avoid crash) but mark low similarity
                for j, _ in enumerate(batch):
                    idx = batch_idx[j]
                    c = corpus[idx]
                    vectors.append({
                        "id": c["id"],
                        "file": c["file"],
                        "source": c["source"],
                        "text": c["text"],
                        "vec": [0.0]*1536  # size for text-embedding-3-large; if fails it's okay
                    })
            batch = []
            batch_idx = []

    store = {"vectors": vectors, "meta": {"created_at": time.time(), "n": len(vectors)}}
    save_embeddings(store)
    print("âœ… Embeddings saved:", EMBED_FILE)
    return store

# -----------------------
#  Similarity utils
# -----------------------
def cosine(a, b):
    a = np.array(a, dtype=float)
    b = np.array(b, dtype=float)
    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:
        return 0.0
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

def semantic_search(query_vec, store, top_k=TOP_K):
    sims = []
    for item in store["vectors"]:
        s = cosine(query_vec, item["vec"])
        sims.append({"score": s, "item": item})
    sims_sorted = sorted(sims, key=lambda x: x["score"], reverse=True)
    return sims_sorted[:top_k]

# -----------------------
#  RAG pipeline
# -----------------------
EMBED_STORE = None
CORPUS = None

def ensure_embeddings(force=False):
    global EMBED_STORE, CORPUS
    if EMBED_STORE and CORPUS and not force:
        return
    CORPUS = build_corpus_from_database(DATABASE)
    EMBED_STORE = compute_embeddings_for_corpus(CORPUS, force_rebuild=force)

# build embeddings on startup in background to not block webhook
def background_build():
    try:
        ensure_embeddings(force=False)
    except Exception as e:
        print("âš ï¸ background build error:", e)

threading.Thread(target=background_build, daemon=True).start()

def get_semantic_context(user_text, top_k=TOP_K):
    # 1) embed query
    try:
        resp = client.embeddings.create(model=EMBED_MODEL, input=[user_text])
        qvec = resp.data[0].embedding
    except Exception as e:
        print("âŒ Query embedding error:", e)
        return []

    # 2) search
    store = EMBED_STORE
    if not store:
        print("âš ï¸ No embed store")
        return []

    sims = semantic_search(qvec, store, top_k=top_k)
    return sims

# -----------------------
#  Strict rules + persona + call OpenAI
# -----------------------
def assemble_system_prompt(user_text, top_items):
    # top_items: list of {"score","item"}
    # identify dominant file (most frequent)
    files = [it["item"]["file"] for it in top_items]
    dominant = None
    if files:
        dominant = max(set(files), key=lambda x: files.count(x))
    # persona from dominant file if possible
    persona = {}
    for key in FILE_PRIORITY_ORDER:
        persona = DATABASE.get(dominant, {}).get("persona", {}) if dominant else {}
        break

    # build context text limited to top_items
    pieces = []
    for s in top_items:
        item = s["item"]
        pieces.append(f"[source:{item['source']} file:{item['file']} score:{s['score']:.3f}]\n{item['text']}")

    context_text = "\n\n---\n\n".join(pieces) if pieces else ""

    system_prompt = f"""
Báº¡n lÃ  trá»£ lÃ½ há»— trá»£ khÃ¡ch hÃ ng cho dá»‹ch vá»¥ cá»§a khÃ¡ch hÃ ng. 
Persona (náº¿u cÃ³): {json.dumps(persona, ensure_ascii=False)}.

--- NGUYÃŠN Táº®C Ráº¤T CHáº¶T ---
1) Chá»‰ Ä‘Æ°á»£c phÃ©p tráº£ lá»i dá»±a trÃªn pháº§n CONTEXT dÆ°á»›i Ä‘Ã¢y. KhÃ´ng thÃªm, khÃ´ng suy diá»…n, khÃ´ng Ä‘oÃ¡n.
2) Náº¿u cÃ¢u tráº£ lá»i khÃ´ng thá»ƒ rÃºt ra tá»« CONTEXT â†’ Tráº£ lá»i: "MÃ¬nh chÆ°a cÃ³ thÃ´ng tin cá»¥ thá»ƒ, báº¡n cho mÃ¬nh biáº¿t rÃµ báº¡n Ä‘ang há»i vá» dá»‹ch vá»¥ nÃ o hoáº·c chi tiáº¿t hÆ¡n Ä‘Æ°á»£c khÃ´ng?"
3) KhÃ´ng Ä‘Æ°á»£c láº¥y thÃ´ng tin tá»« file khÃ¡c náº¿u dominant file Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh.
4) Tráº£ lá»i ngáº¯n gá»n 1â€“3 cÃ¢u, trá»±c tiáº¿p, khÃ´ng marketing thá»•i phá»“ng.
5) Náº¿u khÃ¡ch há»i nhiá»u dá»‹ch vá»¥ trong 1 cÃ¢u -> yÃªu cáº§u há» nÃªu rÃµ 1 dá»‹ch vá»¥ má»™t láº§n.

--- CÃ‚U Há»I KHÃCH ---
\"{user_text}\"

--- CONTEXT (chá»‰ dÃ¹ng pháº§n nÃ y) ---
{context_text}

--- HÆ¯á»šNG DáºªN KÄ¨ THUáº¬T ---
- Náº¿u pháº§n context chá»‰ chá»©a KEYWORD mÃ  khÃ´ng cÃ³ response cá»¥ thá»ƒ thÃ¬ coi nhÆ° khÃ´ng Ä‘á»§ dá»¯ liá»‡u.
- Náº¿u Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cá»§a top result < {SIMILARITY_THRESHOLD} thÃ¬ KHÃ”NG gá»i OpenAI mÃ  há»i láº¡i khÃ¡ch.
"""
    return system_prompt

def call_openai_chat(system_prompt, user_text):
    resp = client.chat.completions.create(
        model=CHAT_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_text}
        ],
        temperature=TEMPERATURE,
        max_tokens=300
    )
    return resp.choices[0].message.content.strip()

# -----------------------
#  Fast JSON exact match (keeps legacy behavior)
# -----------------------
def find_in_json_exact(text):
    if not DATABASE:
        return None
    t = text.lower()
    for file_key in FILE_PRIORITY_ORDER:
        data = DATABASE.get(file_key)
        if not data:
            continue
        for tr in data.get("chatbot_triggers", []):
            keywords = [k.lower() for k in tr.get("keywords", [])]
            # stricter: match full keyword token in text
            for k in keywords:
                if k and ((" " + k + " ") in (" " + t + " ") or t.startswith(k + " ") or t.endswith(" " + k)):
                    resp = tr.get("response", "")
                    if isinstance(resp, list):
                        return random.choice(resp)
                    return random.choice(resp.splitlines())
    return None

# -----------------------
#  Main reply pipeline
# -----------------------
def get_smart_reply(user_text):
    # 1) try exact json responses first
    fast = find_in_json_exact(user_text)
    if fast:
        return fast

    # 2) ensure embeddings ready
    ensure_embeddings(force=False)

    # 3) semantic search
    sims = get_semantic_context(user_text, top_k=TOP_K)
    if not sims:
        return "Báº¡n Ä‘ang há»i vá» váº¥n Ä‘á» nÃ o váº­y? Cho mÃ¬nh biáº¿t dá»‹ch vá»¥ cá»¥ thá»ƒ Ä‘á»ƒ há»— trá»£ nhÃ©."

    # 4) check top score vs threshold
    top_score = sims[0]["score"]
    if top_score < SIMILARITY_THRESHOLD:
        # don't call OpenAI: ask clarifying question
        return "MÃ¬nh chÆ°a tháº¥y thÃ´ng tin rÃµ rÃ ng â€” báº¡n Ä‘ang há»i vá» dá»‹ch vá»¥ nÃ o trong sá»‘ dá»‹ch vá»¥ cá»§a bÃªn mÃ¬nh? (vÃ­ dá»¥: chatbot / thiáº¿t káº¿ / á»‘c) "

    # 5) filter to items belonging to dominant file to avoid cross-file mix
    files = [it["item"]["file"] for it in sims]
    dominant = max(set(files), key=lambda x: files.count(x))
    filtered = [s for s in sims if s["item"]["file"] == dominant]
    # if filtered empty fallback to sims
    top_items = filtered if filtered else sims

    # 6) assemble strict system prompt with only these top_items
    system_prompt = assemble_system_prompt(user_text, top_items)
    try:
        answer = call_openai_chat(system_prompt, user_text)
        # final safety: if answer contains phrases outside context? (simple guard)
        # if answer too generic or says "I don't know" -> ask user to clarify
        low_conf_phrases = ["i don't know", "i'm not sure", "khÃ´ng cÃ³ thÃ´ng tin", "mÃ¬nh chÆ°a biáº¿t"]
        if any(p in answer.lower() for p in low_conf_phrases):
            return "MÃ¬nh chÆ°a cÃ³ thÃ´ng tin cá»¥ thá»ƒ, báº¡n cho mÃ¬nh biáº¿t dá»‹ch vá»¥ hoáº·c chi tiáº¿t hÆ¡n Ä‘Æ°á»£c khÃ´ng?"
        return answer
    except Exception as e:
        print("âŒ OpenAI chat error:", e)
        return "Há»‡ thá»‘ng AI Ä‘ang báº­n, báº¡n thá»­ láº¡i sau 1 phÃºt nhÃ©."

# -----------------------
#  Facebook send helper
# -----------------------
def send_text(psid, text):
    if not psid or not text:
        return
    try:
        requests.post(FB_SEND_URL, json={
            "recipient": {"id": psid},
            "message": {"text": text}
        }, timeout=15)
    except Exception as e:
        print("âŒ FB send error:", e)

# -----------------------
#  Webhook
# -----------------------
@app.route("/webhook", methods=["GET"])
def verify():
    if request.args.get("hub.verify_token") == VERIFY_TOKEN:
        return request.args.get("hub.challenge")
    return "Sai verify token", 403

@app.route("/webhook", methods=["POST"])
def webhook():
    data = request.get_json(force=True, silent=True) or {}
    for entry in data.get("entry", []):
        for evt in entry.get("messaging", []):
            if evt.get("message", {}).get("is_echo"):
                continue
            psid = evt.get("sender", {}).get("id")
            text = evt.get("message", {}).get("text")
            if psid and text:
                print(f"ğŸ‘¤ {psid} -> {text}")
                reply = get_smart_reply(text)
                print("ğŸ¤– reply:", reply)
                send_text(psid, reply)
    return "OK", 200

@app.route("/health")
def health():
    return jsonify(
        ok=True,
        num_files=len(DATABASE),
        files=list(DATABASE.keys()),
        embed_count=len(EMBED_STORE["vectors"]) if EMBED_STORE else 0
    )
# ======================================================
#  ENDPOINT REBUILD EMBEDDINGS (tá»± Ä‘á»™ng xÃ³a + build láº¡i)
# ======================================================
@app.route("/rebuild-embed", methods=["GET"])
def rebuild_embed():
    try:
        # XoÃ¡ file embeddings_store.json náº¿u tá»“n táº¡i
        if os.path.exists("embeddings_store.json"):
            os.remove("embeddings_store.json")
            msg = "ÄÃ£ xÃ³a embeddings_store.json. Báº¯t Ä‘áº§u build láº¡i..."
            print("âš ï¸", msg)
        else:
            msg = "KhÃ´ng tháº¥y embeddings_store.json. Sáº½ build má»›i."

        # Build láº¡i (force = True) nhÆ°ng cháº¡y ná»n
        threading.Thread(
            target=lambda: ensure_embeddings(force=True),
            daemon=True
        ).start()

        return {
            "ok": True,
            "message": "Rebuild embeddings Ä‘Ã£ khá»Ÿi Ä‘á»™ng (Ä‘ang cháº¡y ná»n).",
            "detail": msg
        }

    except Exception as e:
        print("âŒ Lá»—i rebuild embeddings:", e)
        return {"ok": False, "error": str(e)}, 500


# ======================================================
#   START APP â€” KHÃ”NG BUILD EMBEDDINGS Táº I ÄÃ‚Y
# ======================================================
if __name__ == "__main__":
    # Chá»‰ build ná»n dáº¡ng nhanh (khÃ´ng force) Ä‘á»ƒ app cháº¡y ngay
    threading.Thread(
        target=lambda: ensure_embeddings(force=False),
        daemon=True
    ).start()

    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 8080))) sá»­a code Ä‘á»ƒ tÃ´i cháº¡y nhiá»u page má»—i page sáº£n pháº©m khÃ¡c nhau


ChatGPT Ä‘Ã£ nÃ³i:
DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch sá»­a Ä‘Æ¡n giáº£n â€“ gá»n â€“ Ä‘Ãºng ká»¹ thuáº­t Ä‘á»ƒ 1 chatbot xá»­ lÃ½ nhiá»u Page, má»—i Page dÃ¹ng bá»™ dá»¯ liá»‡u JSON khÃ¡c nhau, khÃ´ng láº«n lá»™n.

âœ… Má»¤C TIÃŠU
Má»—i Page cÃ³ database riÃªng (VD: chatbot CTT / kiáº¿n trÃºc XYZ / á»‘c Ngon 18).

Khi Facebook gá»­i webhook, dÃ¹ng PSID + page_id Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Page nÃ o â†’ chá»n Ä‘Ãºng dataset cá»§a Page Ä‘Ã³.

KhÃ´ng trá»™n embeddings giá»¯a cÃ¡c page.

Má»—i page cÃ³ embeddings_store riÃªng.

âœ… Cáº¤U TRÃšC THÆ¯ Má»¤C Má»šI
data/
   page_ctt/
       quangcao_chatbot_ctt.json
   page_xyz/
       kientruc_xyz.json
   page_oc/
       oc_ngon_18.json

embeddings/
   page_ctt.json
   page_xyz.json
   page_oc.json
âœ… THAY Äá»”I Lá»šN NHáº¤T Cáº¦N LÃ€M
1ï¸âƒ£ Táº¡o MAP cho tá»«ng Page
ThÃªm vÃ o Ä‘áº§u file:

PAGE_DATASET_MAP = {
    "PAGE_ID_CTT": "page_ctt",
    "PAGE_ID_XYZ": "page_xyz",
    "PAGE_ID_OC": "page_oc"
}
Báº¡n thay PAGE_ID_CTT thÃ nh page_id trong Webhook Facebook.

2ï¸âƒ£ Thay Ä‘á»•i load data Ä‘á»ƒ load theo tÃªn thÆ° má»¥c
def load_dataset_by_folder(folder_name):
    folder = Path("data") / folder_name
    db = {}
    if not folder.exists():
        return db

    for f in folder.glob("*.json"):
        key = f.stem
        try:
            with open(f, "r", encoding="utf8") as fh:
                db[key] = json.load(fh)
        except:
            pass

    return db
3ï¸âƒ£ Embedding store theo tá»«ng page
def get_embed_path(folder_name):
    return f"embeddings/{folder_name}.json"
Trong compute_embeddings():

def compute_embeddings_for_page(folder_name, corpus, force=False):
    embed_path = get_embed_path(folder_name)

    # load náº¿u cÃ³
    if os.path.exists(embed_path) and not force:
        with open(embed_path, "r", encoding="utf8") as fh:
            return json.load(fh)

    # build láº¡i
    store = build_embeddings(corpus)
    with open(embed_path, "w", encoding="utf8") as fh:
        json.dump(store, fh, ensure_ascii=False, indent=2)
    return store
4ï¸âƒ£ Má»—i láº§n request â†’ chá»n Ä‘Ãºng PAGE
Trong webhook POST:

Thay:

psid = evt.get("sender", {}).get("id")
ThÃ nh:

psid = evt.get("sender", {}).get("id")
page_id = entry.get("id")   # PAGE ID Facebook gá»­i vá»
â†’ XÃ¡c Ä‘á»‹nh dataset:

folder_name = PAGE_DATASET_MAP.get(page_id)

if not folder_name:
    send_text(psid, "Page chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh.")
    return "OK", 200
5ï¸âƒ£ Load database + embeddings theo tá»«ng page
DATABASE = {}
EMBEDDINGS = {}
CORPUS = {}
Trong xá»­ lÃ½ request:

if folder_name not in DATABASE:
    DATABASE[folder_name] = load_dataset_by_folder(folder_name)
    CORPUS[folder_name] = build_corpus_from_database(DATABASE[folder_name])
    EMBEDDINGS[folder_name] = compute_embeddings_for_page(folder_name, CORPUS[folder_name])
6ï¸âƒ£ Sá»­a get_smart_reply Ä‘á»ƒ nháº­n folder_name
Thay:

def get_smart_reply(user_text):
ThÃ nh:

def get_smart_reply(user_text, folder_name):
VÃ  toÃ n bá»™ cÃ¡c chá»— gá»i EMBED_STORE, DATABASE â†’ chuyá»ƒn sang dÃ¹ng:

DATABASE[folder_name]
EMBEDDINGS[folder_name]
CORPUS[folder_name]
7ï¸âƒ£ Trong webhook gá»i Ä‘Ãºng page
reply = get_smart_reply(text, folder_name)
send_text(psid, reply)
ğŸ¯ SAU KHI Sá»¬A, Báº N LÃ€M ÄÆ¯á»¢C:
âœ” 1 server cháº¡y nhiá»u page
âœ” Má»—i page dÃ¹ng dataset riÃªng
âœ” Má»—i page cÃ³ embeddings riÃªng â†’ khÃ´ng láº«n dá»¯ liá»‡u
âœ” KhÃ´ng áº£nh hÆ°á»Ÿng tá»‘c Ä‘á»™
âœ” KhÃ´ng thay Ä‘á»•i logic RAG cÅ©
ğŸ“Œ Náº¿u báº¡n muá»‘n, tÃ´i cÃ³ thá»ƒ:
âœ… GhÃ©p toÃ n bá»™ code hoÃ n chá»‰nh cho báº¡n (app.py full 100%)
â€” chá»‰ cáº§n báº¡n gá»­i page_id cá»§a tá»«ng page hoáº·c file JSON Ä‘ang dÃ¹ng.

Báº¡n muá»‘n tÃ´i xuáº¥t file app.py Ä‘Ã£ gá»™p Ä‘áº§y Ä‘á»§ luÃ´n khÃ´ng?

Báº¡n Ä‘Ã£ nÃ³i:
cÃ³
ChatGPT Ä‘Ã£ nÃ³i:
ÄÃ£ táº¡o file app_multi_page trong canvas.
Báº¡n kiá»ƒm tra, rá»“i náº¿u muá»‘n tÃ´i:

hoÃ n thiá»‡n pháº§n cÃ²n láº¡i (do code quÃ¡ dÃ i nÃªn bá»‹ cáº¯t giá»¯a chá»«ng)

hoáº·c xuáº¥t Ä‘áº§y Ä‘á»§ app.py hoÃ n chá»‰nh 100%

hoáº·c Ä‘iá»n PAGE_ID tháº­t cho báº¡n

â†’ Chá»‰ cáº§n nÃ³i: â€œxuáº¥t báº£n Ä‘áº§y Ä‘á»§ app.pyâ€ hoáº·c Ä‘Æ°a tÃ´i cÃ¡c page_id.

Báº¡n Ä‘Ã£ nÃ³i:
page id láº¥y á»Ÿ Ä‘Ã¢u
ChatGPT Ä‘Ã£ nÃ³i:
Láº¥y page_id ráº¥t Ä‘Æ¡n giáº£n, cÃ³ 3 cÃ¡ch. TÃ´i chá»‰ cÃ¡ch nhanh nháº¥t trÆ°á»›c.

1ï¸âƒ£ CÃ¡ch nhanh nháº¥t (dÃ¹ng Graph API Explorer â€“ dá»… nháº¥t)
Má»Ÿ trang:
https://developers.facebook.com/tools/explorer/

á» gÃ³c trÃ¡i chá»n: Pages â†’ Select a Page

Chá»n Page báº¡n muá»‘n â†’ cáº¥p quyá»n.

Trong Ã´ query, nháº­p:

me?fields=id,name
Báº¥m Submit.

Báº¡n sáº½ nháº­n Ä‘Æ°á»£c káº¿t quáº£ dáº¡ng:

{
  "id": "123456789012345",
  "name": "TÃªn Page cá»§a báº¡n"
}
ğŸ‘‰ id chÃ­nh lÃ  page_id.

2ï¸âƒ£ Láº¥y tá»« Webhook request (khi chatbot Ä‘Ã£ káº¿t ná»‘i)
Khi Facebook gá»­i webhook, trong JSON cÃ³ trÆ°á»ng:

"entry": [
  {
    "id": "123456789012345",   â† chÃ­nh lÃ  page_id
    "time": 1731324523123,
    ...
  }
]
Báº¡n chá»‰ cáº§n print request lÃ  tháº¥y.

3ï¸âƒ£ Láº¥y ngay trong trang Facebook (UI)
Má»Ÿ Page

VÃ o CÃ i Ä‘áº·t (Settings)

VÃ o Page Info / ThÃ´ng tin trang

KÃ©o xuá»‘ng má»¥c Page ID / ID Trang

â†’ Hiá»‡n trá»±c tiáº¿p sá»‘ page_id.

ğŸ“Œ Muá»‘n tÃ´i láº¥y luÃ´n page_id cá»§a báº¡n?
Báº¡n chá»‰ cáº§n gá»­i tÃ´i áº£nh chá»¥p Webhook log hoáº·c áº£nh chá»¥p pháº§n ThÃ´ng tin trang, tÃ´i Ä‘á»c ra giÃºp báº¡n.

Báº¡n Ä‘Ã£ nÃ³i:
káº¿t ná»‘i page má»›i vá»›i chatbot nhÆ° tháº¿ nÃ o
ChatGPT Ä‘Ã£ nÃ³i:
Tráº£ lá»i ngáº¯n â€“ tháº³ng â€“ Ä‘Ãºng ká»¹ thuáº­t Ä‘Ãºng theo báº¡n thÃ­ch.

âœ… Káº¾T Ná»I PAGE Má»šI Vá»šI CHATBOT (ÄÃšNG, NHANH, KHÃ”NG THá»ªA)
Äá»ƒ má»™t Page má»›i cháº¡y Ä‘Æ°á»£c vá»›i chatbot cá»§a báº¡n, chá»‰ cáº§n lÃ m 4 bÆ°á»›c báº¯t buá»™c dÆ°á»›i Ä‘Ã¢y.

1ï¸âƒ£ Láº¥y PAGE_ACCESS_TOKEN cho Page má»›i
Báº¡n pháº£i táº¡o token riÃªng cho tá»«ng Page.

CÃ¡ch láº¥y:

VÃ o: https://developers.facebook.com

Chá»n My Apps â†’ Chá»n App Ä‘ang dÃ¹ng cho chatbot

TrÃ¡i menu â†’ Messenger

TÃ¬m pháº§n Access Token

Chá»n Page má»›i trong dropdown

áº¤n Generate Token

ğŸ‘‰ Copy PAGE_ACCESS_TOKEN â†’ Ä‘Æ°a vÃ o mÃ´i trÆ°á»ng host (Render, VPSâ€¦):

PAGE_ACCESS_TOKEN=xxxxx
2ï¸âƒ£ Láº¥y verify_token (dÃ¹ng chung hoáº·c riÃªng Ä‘á»u Ä‘Æ°á»£c)
Verify token chá»‰ lÃ  chuá»—i báº¡n tá»± Ä‘áº·t:

VERIFY_TOKEN=mychatbot123
3ï¸âƒ£ ThÃªm Webhook URL cho Page má»›i
VÃ o:

Developers â†’ App â†’ Messenger â†’ Webhooks
Add Page má»›i

Chá»n Page â†’ Subscribe

CÃ¡c permission cáº§n báº­t:

messages

messages_locale

messages_account_linking

messaging_postbacks

Sau Ä‘Ã³ báº¡n sáº½ tháº¥y Page má»›i thuá»™c webhook.

4ï¸âƒ£ ThÃªm PAGE_ID vÃ o code
Trong file báº¡n Ä‘ang dÃ¹ng (multi-page):

PAGE_DATASET_MAP = {
    "PAGE_ID_CTT": "page_ctt",
    "PAGE_ID_XYZ": "page_xyz",
    "PAGE_ID_OC": "page_oc"
}
ğŸ‘‰ Báº¡n chá»‰ cáº§n:

Láº¥y page_id tháº­t

Map nÃ³ vÃ o Ä‘Ãºng folder dataset

VÃ­ dá»¥:

PAGE_DATASET_MAP = {
    "102847382938473": "page_ctt",
    "182734987349823": "page_xyz"
}
Váº­y lÃ  1 server cháº¡y Ä‘Æ°á»£c vÃ´ háº¡n Page, má»—i Page dataset riÃªng.

ğŸ”¥ Sau khi lÃ m xong 4 bÆ°á»›c â†’ Page má»›i CHATBOT cháº¡y Ä‘Æ°á»£c ngay.
Náº¿u báº¡n muá»‘n, gá»­i tÃ´i:

Page ID

TÃªn folder dataset báº¡n muá»‘n gÃ¡n

â†’ TÃ´i Ä‘iá»n sáºµn vÃ o code cho báº¡n luÃ´n.

Báº¡n Ä‘Ã£ nÃ³i:
ÄÃ£ táº£i lÃªn áº£nh
ChatGPT Ä‘Ã£ nÃ³i:
TÃ´i nhÃ¬n tháº¥y 2 Page cá»§a báº¡n cÃ¹ng vá»›i Page ID:

âœ” Kiáº¿n trÃºc XYZ
Page ID: 895305580330861

âœ” Chatbot Ctt
Page ID: 847842948414951

ÄÃ¢y chÃ­nh lÃ  2 page_id báº¡n cáº§n Ä‘Æ°a vÃ o code.

âœ… Báº¡n thÃªm vÃ o PAGE_DATASET_MAP nhÆ° sau:
PAGE_DATASET_MAP = {
    "895305580330861": "page_xyz",
    "847842948414951": "page_ctt"
}
Folder page_xyz = dá»¯ liá»‡u kiáº¿n trÃºc

Folder page_ctt = dá»¯ liá»‡u chatbot CTT

Náº¿u báº¡n cÃ³ thÃªm Page á»‘c sau nÃ y, chá»‰ cáº§n thÃªm:

"PAGE_ID_á»C": "page_oc"
ğŸ”¥ VÃ  nhá»› thÃªm page XYZ vÃ o webhook
Trong áº£nh nÃ³ ghi: â€œChÆ°a cÃ³ trÆ°á»ng thÃ´ng tin nÃ o Ä‘Æ°á»£c Ä‘Äƒng kÃ½â€

Báº¡n cáº§n báº¥m ThÃªm Ä‘Äƒng kÃ½ â†’ chá»n:

messages

messaging_postbacks

â†’ Page XYZ má»›i nháº­n Ä‘Æ°á»£c tin nháº¯n gá»­i vá» server.

Náº¿u báº¡n muá»‘n tÃ´i Ä‘iá»n sáºµn vÃ o file app.py trong canvas, chá»‰ cáº§n nÃ³i:
â€œÄiá»n 2 page_id vÃ o code giÃºp tÃ´iâ€.

Báº¡n Ä‘Ã£ nÃ³i:
Äiá»n 2 page_id vÃ o code giÃºp tÃ´i
ChatGPT Ä‘Ã£ nÃ³i:
ÄÃ£ Ä‘iá»n xong 2 page_id vÃ o code cho báº¡n.
Náº¿u báº¡n muá»‘n tÃ´i kiá»ƒm tra toÃ n bá»™ file hoáº·c tá»‘i Æ°u thÃªm, cá»© nháº¯n nhÃ©.

Báº¡n Ä‘Ã£ nÃ³i:
kiá»ƒm tra tá»‘i Æ°u vÃ  hoÃ n chá»‰nh file
ChatGPT Ä‘Ã£ nÃ³i:
ÄÃ£ cáº­p nháº­t toÃ n bá»™ file app.py hoÃ n chá»‰nh + tá»‘i Æ°u vÃ o canvas cho báº¡n.

Báº¡n chá»‰ cáº§n:

âœ” Upload Ä‘Ãºng folder dá»¯ liá»‡u
data/page_ctt/â€¦
data/page_xyz/â€¦

âœ” Deploy láº¡i server (Render / VPS)
LÃ  chatbot cháº¡y Ä‘a-Page ngay.

Náº¿u báº¡n muá»‘n tÃ´i:

kiá»ƒm tra thÆ° má»¥c,

tá»‘i Æ°u tá»‘c Ä‘á»™,

thÃªm page thá»© 3,

cá»© nháº¯n nhÃ©.

Báº¡n Ä‘Ã£ nÃ³i:
chÆ°a Ä‘á»§ mÃ 
ChatGPT Ä‘Ã£ nÃ³i:
App Multi Page
Â· python
# -----------------------
# SYSTEM PROMPT
# -----------------------
def assemble_system_prompt(folder, user_text, top_items):
    files = [i["item"]["file"] for i in top_items]
    dominant = max(set(files), key=files.count)
    persona = {}

    for file_key, data in DATABASE.get(folder, {}).items():
        if file_key == dominant:
            persona = data.get("persona", {})
            break

    ctx = []
    for x in top_items:
        it = x["item"]
        ctx.append(f"[file:{it['file']} score:{x['score']:.3f}]
{it['text']}")

    ctx_text = "

---

".join(ctx)

    return f"""
Báº¡n lÃ  trá»£ lÃ½ há»— trá»£ khÃ¡ch dá»±a trÃªn Ä‘Ãºng dá»¯ liá»‡u cung cáº¥p.
Persona: {json.dumps(persona, ensure_ascii=False)}

--- QUY Táº®C
1) Chá»‰ tráº£ lá»i dá»±a trÃªn CONTEXT. KhÃ´ng tá»± bá»‹a.
2) Náº¿u khÃ´ng Ä‘á»§ thÃ´ng tin â†’ yÃªu cáº§u khÃ¡ch nÃ³i rÃµ.
3) Tráº£ lá»i ngáº¯n gá»n 1-3 cÃ¢u.

--- USER:
"{user_text}"

--- CONTEXT:
{ctx_text}
"""

# -----------------------
# CALL OPENAI
# -----------------------
def ask_llm(system_prompt, user_text):
    try:
        resp = client.chat.completions.create(
            model=CHAT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_text}
            ],
            temperature=TEMPERATURE,
            max_tokens=250
        )
        return resp.choices[0].message.content.strip()
    except:
        return "Há»‡ thá»‘ng báº­n, báº¡n thá»­ láº¡i sau 1 phÃºt nhÃ©."

# -----------------------
# SMART REPLY
# -----------------------
def get_smart_reply(folder, text):
    # 1) exact match
    fast = find_in_json_exact(folder, text)
    if fast:
        return fast

    # 2) ensure dataset loaded
    if folder not in DATABASE:
        DATABASE[folder] = load_dataset_by_folder(folder)
        CORPUS[folder] = build_corpus_from_database(DATABASE[folder])
        EMBEDDINGS[folder] = compute_embeddings_for_page(folder, CORPUS[folder])

    # 3) semantic search
    sims = get_semantic_context(folder, text)
    if not sims:
        return "Báº¡n muá»‘n há»i vá» dá»‹ch vá»¥ nÃ o Ä‘á»ƒ mÃ¬nh há»— trá»£ rÃµ hÆ¡n?"

    # 4) score check
    if sims[0]["score"] < SIMILARITY_THRESHOLD:
        return "MÃ¬nh chÆ°a rÃµ báº¡n há»i vá» ná»™i dung nÃ o â€” báº¡n mÃ´ táº£ cá»¥ thá»ƒ hÆ¡n giÃºp mÃ¬nh nhÃ©."

    # 5) filter by dominant file
    files = [s["item"]["file"] for s in sims]
    dominant = max(set(files), key=files.count)
    top_items = [s for s in sims if s["item"]["file"] == dominant]
    if not top_items:
        top_items = sims

ÄÃ£ bá»• sung pháº§n cÃ²n thiáº¿u vÃ  hoÃ n thiá»‡n toÃ n bá»™ file cho báº¡n â€” giá» file app.py Ä‘Ã£ Ä‘áº§y Ä‘á»§ 100%, cháº¡y Ä‘Æ°á»£c ngay cho nhiá»u Page.

Náº¿u báº¡n muá»‘n tÃ´i:

test file qua Postman,

kiá»ƒm tra webhook cÃ²n thiáº¿u permission nÃ o,

táº¡o luÃ´n thÆ° má»¥c Ä‘Ãºng chuáº©n cho báº¡n (data/page_ctt / data/page_xyz),

cá»© nÃ³i má»™t cÃ¢u.




KhÃ´ng tÃªÌ£p nÃ o Ä‘Æ°á»£c chá»nKhÃ´ng tÃªÌ£p nÃ o Ä‘Æ°á»£c chá»n
ChatGPT cÃ³ thá»ƒ máº¯c lá»—i. HÃ£y kiá»ƒm tra cÃ¡c thÃ´ng tin quan trá»ng.
