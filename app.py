# app.py
import os
import json
import math
import time
import random
import requests
import threading
from pathlib import Path
from tqdm import tqdm
import numpy as np
from flask import Flask, request, jsonify
from openai import OpenAI

# -----------------------
#  CONFIG
# -----------------------
EMBED_MODEL = "text-embedding-3-large"
CHAT_MODEL = "gpt-4o-mini"
EMBED_BATCH = 16  # n·∫øu c√≥ nhi·ªÅu text, chia batch
EMBED_FILE = "embeddings_store.json"
CHUNK_SIZE = 400  # k√Ω t·ª± tr√™n chunk (t√πy ch·ªânh)
SIMILARITY_THRESHOLD = 0.72  # n·∫øu score th·∫•p h∆°n -> h·ªèi l·∫°i
TOP_K = 5
TEMPERATURE = 0.12

PAGE_TOKEN = os.environ.get("PAGE_ACCESS_TOKEN", "")
VERIFY_TOKEN = os.environ.get("VERIFY_TOKEN", "")
OPENAI_KEY = os.environ.get("OPENAI_API_KEY", "")

FB_SEND_URL = f"https://graph.facebook.com/v19.0/me/messages?access_token={PAGE_TOKEN}"

app = Flask(__name__)

# -----------------------
#  OpenAI client
# -----------------------
try:
    client = OpenAI(api_key=OPENAI_KEY)
    print("‚úÖ OpenAI client ready")
except Exception as e:
    print("‚ùå OpenAI init error:", e)
    client = None

# -----------------------
#  Load JSON dataset
# -----------------------
DATA_FOLDER = Path("data")
FILE_PRIORITY_ORDER = [
    "quangcao_chatbot_ctt",
    "kientruc_xyz",
    "oc_ngon_18"
]

def load_all_data(folder=DATA_FOLDER):
    db = {}
    if not folder.exists():
        print("‚ùå data/ folder missing")
        return db
    for f in folder.glob("*.json"):
        try:
            key = f.stem
            with open(f, "r", encoding="utf8") as fh:
                db[key] = json.load(fh)
        except Exception as e:
            print("‚ùå load fail", f, e)
    print("üìÇ Loaded:", list(db.keys()))
    return db

DATABASE = load_all_data()

# -----------------------
#  Chunking & Indexing
# -----------------------
def text_to_chunks(text, size=CHUNK_SIZE):
    text = text.strip()
    if not text:
        return []
    # chia theo c√¢u g·∫ßn ƒë√∫ng r·ªìi gh√©p cho ƒë·ªß chunk
    parts = text.replace("\n", " ").split(". ")
    chunks = []
    cur = ""
    for p in parts:
        if len(cur) + len(p) + 2 <= size:
            cur = (cur + ". " + p).strip(" .")
        else:
            if cur:
                chunks.append(cur.strip())
            cur = p
    if cur:
        chunks.append(cur.strip())
    # n·∫øu v·∫´n c√≥ chunk qu√° d√†i th√¨ c·∫Øt tr·ª±c ti·∫øp
    final = []
    for c in chunks:
        if len(c) <= size:
            final.append(c)
        else:
            for i in range(0, len(c), size):
                final.append(c[i:i+size])
    return final

def build_corpus_from_database(db):
    """
    T·∫°o danh s√°ch chunk dict:
    { "id": str, "file": file_key, "source": "trigger|product|project|persona",
      "text": chunk_text }
    """
    corpus = []
    idx = 0
    for file_key, content in db.items():
        # triggers: th∆∞·ªùng ch·ª©a responses and keywords
        for tr in content.get("chatbot_triggers", []):
            # include keywords + response text
            keywords = " ".join(tr.get("keywords", []))
            resp = tr.get("response", "")
            if isinstance(resp, list):
                resp = " ".join(resp)
            text = f"KEYWORDS: {keywords}\nRESPONSE: {resp}"
            for chunk in text_to_chunks(text):
                corpus.append({
                    "id": f"c_{idx}",
                    "file": file_key,
                    "source": "trigger",
                    "text": chunk
                })
                idx += 1

        # products
        for p in content.get("products", []):
            name = p.get("name", "")
            desc = p.get("description", "") if isinstance(p.get("description", ""), str) else json.dumps(p.get("description", ""))
            text = f"PRODUCT: {name}\n{desc}"
            for chunk in text_to_chunks(text):
                corpus.append({
                    "id": f"c_{idx}",
                    "file": file_key,
                    "source": "product",
                    "text": chunk
                })
                idx += 1

        # projects
        for pr in content.get("highlight_projects", []):
            name = pr.get("name", "")
            desc = pr.get("summary", "") if isinstance(pr.get("summary", ""), str) else json.dumps(pr.get("summary", ""))
            text = f"PROJECT: {name}\n{desc}"
            for chunk in text_to_chunks(text):
                corpus.append({
                    "id": f"c_{idx}",
                    "file": file_key,
                    "source": "project",
                    "text": chunk
                })
                idx += 1

        # persona (short)
        persona = content.get("persona", {})
        if persona:
            text = f"PERSONA: {persona.get('role','')}. {persona.get('tone','')}. Goal: {persona.get('goal','')}"
            for chunk in text_to_chunks(text):
                corpus.append({
                    "id": f"c_{idx}",
                    "file": file_key,
                    "source": "persona",
                    "text": chunk
                })
                idx += 1

    return corpus

# -----------------------
#  Embedding store (disk)
# -----------------------
def load_embeddings(path=EMBED_FILE):
    if not Path(path).exists():
        return None
    with open(path, "r", encoding="utf8") as fh:
        return json.load(fh)

def save_embeddings(store, path=EMBED_FILE):
    with open(path, "w", encoding="utf8") as fh:
        json.dump(store, fh, ensure_ascii=False, indent=2)

def compute_embeddings_for_corpus(corpus, force_rebuild=False):
    """
    corpus: list of chunk dicts
    returns store: {"vectors": [{"id":..., "vec":[...], "file":..., "source":..., "text":...}], "meta": {...}}
    """
    existing = load_embeddings()
    if existing and not force_rebuild:
        # quick sanity: if size matches corpus length use it
        if len(existing.get("vectors", [])) == len(corpus):
            print("üóÑÔ∏è Load embeddings from disk")
            return existing
        else:
            print("‚ö†Ô∏è Embedding count mismatch. Rebuilding.")

    print("‚öôÔ∏è Creating embeddings for corpus (this may take a while)...")
    vectors = []
    texts = [c["text"] for c in corpus]
    batch = []
    batch_idx = []
    for i, txt in enumerate(tqdm(texts, desc="chunks")):
        batch.append(txt)
        batch_idx.append(i)
        if len(batch) >= EMBED_BATCH or i == len(texts) - 1:
            # call embeddings
            try:
                resp = client.embeddings.create(model=EMBED_MODEL, input=batch)
                # SDK response assumed resp.data[*].embedding
                for j, out in enumerate(resp.data):
                    emb = out.embedding
                    idx = batch_idx[j]
                    c = corpus[idx]
                    vectors.append({
                        "id": c["id"],
                        "file": c["file"],
                        "source": c["source"],
                        "text": c["text"],
                        "vec": emb
                    })
            except Exception as e:
                print("‚ùå Embedding API error:", e)
                # fallback: zero vector (avoid crash) but mark low similarity
                for j, _ in enumerate(batch):
                    idx = batch_idx[j]
                    c = corpus[idx]
                    vectors.append({
                        "id": c["id"],
                        "file": c["file"],
                        "source": c["source"],
                        "text": c["text"],
                        "vec": [0.0]*1536  # size for text-embedding-3-large; if fails it's okay
                    })
            batch = []
            batch_idx = []

    store = {"vectors": vectors, "meta": {"created_at": time.time(), "n": len(vectors)}}
    save_embeddings(store)
    print("‚úÖ Embeddings saved:", EMBED_FILE)
    return store

# -----------------------
#  Similarity utils
# -----------------------
def cosine(a, b):
    a = np.array(a, dtype=float)
    b = np.array(b, dtype=float)
    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:
        return 0.0
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

def semantic_search(query_vec, store, top_k=TOP_K):
    sims = []
    for item in store["vectors"]:
        s = cosine(query_vec, item["vec"])
        sims.append({"score": s, "item": item})
    sims_sorted = sorted(sims, key=lambda x: x["score"], reverse=True)
    return sims_sorted[:top_k]

# -----------------------
#  RAG pipeline
# -----------------------
EMBED_STORE = None
CORPUS = None

def ensure_embeddings(force=False):
    global EMBED_STORE, CORPUS
    if EMBED_STORE and CORPUS and not force:
        return
    CORPUS = build_corpus_from_database(DATABASE)
    EMBED_STORE = compute_embeddings_for_corpus(CORPUS, force_rebuild=force)

# build embeddings on startup in background to not block webhook
def background_build():
    try:
        ensure_embeddings(force=False)
    except Exception as e:
        print("‚ö†Ô∏è background build error:", e)

threading.Thread(target=background_build, daemon=True).start()

def get_semantic_context(user_text, top_k=TOP_K):
    # 1) embed query
    try:
        resp = client.embeddings.create(model=EMBED_MODEL, input=[user_text])
        qvec = resp.data[0].embedding
    except Exception as e:
        print("‚ùå Query embedding error:", e)
        return []

    # 2) search
    store = EMBED_STORE
    if not store:
        print("‚ö†Ô∏è No embed store")
        return []

    sims = semantic_search(qvec, store, top_k=top_k)
    return sims

# -----------------------
#  Strict rules + persona + call OpenAI
# -----------------------
def assemble_system_prompt(user_text, top_items):
    # top_items: list of {"score","item"}
    # identify dominant file (most frequent)
    files = [it["item"]["file"] for it in top_items]
    dominant = None
    if files:
        dominant = max(set(files), key=lambda x: files.count(x))
    # persona from dominant file if possible
    persona = {}
    for key in FILE_PRIORITY_ORDER:
        persona = DATABASE.get(dominant, {}).get("persona", {}) if dominant else {}
        break

    # build context text limited to top_items
    pieces = []
    for s in top_items:
        item = s["item"]
        pieces.append(f"[source:{item['source']} file:{item['file']} score:{s['score']:.3f}]\n{item['text']}")

    context_text = "\n\n---\n\n".join(pieces) if pieces else ""

    system_prompt = f"""
B·∫°n l√† tr·ª£ l√Ω h·ªó tr·ª£ kh√°ch h√†ng cho d·ªãch v·ª• c·ªßa kh√°ch h√†ng. 
Persona (n·∫øu c√≥): {json.dumps(persona, ensure_ascii=False)}.

--- NGUY√äN T·∫ÆC R·∫§T CH·∫∂T ---
1) Ch·ªâ ƒë∆∞·ª£c ph√©p tr·∫£ l·ªùi d·ª±a tr√™n ph·∫ßn CONTEXT d∆∞·ªõi ƒë√¢y. Kh√¥ng th√™m, kh√¥ng suy di·ªÖn, kh√¥ng ƒëo√°n.
2) N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng th·ªÉ r√∫t ra t·ª´ CONTEXT ‚Üí Tr·∫£ l·ªùi: "M√¨nh ch∆∞a c√≥ th√¥ng tin c·ª• th·ªÉ, b·∫°n cho m√¨nh bi·∫øt r√µ b·∫°n ƒëang h·ªèi v·ªÅ d·ªãch v·ª• n√†o ho·∫∑c chi ti·∫øt h∆°n ƒë∆∞·ª£c kh√¥ng?"
3) Kh√¥ng ƒë∆∞·ª£c l·∫•y th√¥ng tin t·ª´ file kh√°c n·∫øu dominant file ƒë√£ ƒë∆∞·ª£c x√°c ƒë·ªãnh.
4) Tr·∫£ l·ªùi ng·∫Øn g·ªçn 1‚Äì3 c√¢u, tr·ª±c ti·∫øp, kh√¥ng marketing th·ªïi ph·ªìng.
5) N·∫øu kh√°ch h·ªèi nhi·ªÅu d·ªãch v·ª• trong 1 c√¢u -> y√™u c·∫ßu h·ªç n√™u r√µ 1 d·ªãch v·ª• m·ªôt l·∫ßn.

--- C√ÇU H·ªéI KH√ÅCH ---
\"{user_text}\"

--- CONTEXT (ch·ªâ d√πng ph·∫ßn n√†y) ---
{context_text}

--- H∆Ø·ªöNG D·∫™N Kƒ® THU·∫¨T ---
- N·∫øu ph·∫ßn context ch·ªâ ch·ª©a KEYWORD m√† kh√¥ng c√≥ response c·ª• th·ªÉ th√¨ coi nh∆∞ kh√¥ng ƒë·ªß d·ªØ li·ªáu.
- N·∫øu ƒë·ªô t∆∞∆°ng ƒë·ªìng c·ªßa top result < {SIMILARITY_THRESHOLD} th√¨ KH√îNG g·ªçi OpenAI m√† h·ªèi l·∫°i kh√°ch.
"""
    return system_prompt

def call_openai_chat(system_prompt, user_text):
    resp = client.chat.completions.create(
        model=CHAT_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_text}
        ],
        temperature=TEMPERATURE,
        max_tokens=300
    )
    return resp.choices[0].message.content.strip()

# -----------------------
#  Fast JSON exact match (keeps legacy behavior)
# -----------------------
def find_in_json_exact(text):
    if not DATABASE:
        return None
    t = text.lower()
    for file_key in FILE_PRIORITY_ORDER:
        data = DATABASE.get(file_key)
        if not data:
            continue
        for tr in data.get("chatbot_triggers", []):
            keywords = [k.lower() for k in tr.get("keywords", [])]
            # stricter: match full keyword token in text
            for k in keywords:
                if k and ((" " + k + " ") in (" " + t + " ") or t.startswith(k + " ") or t.endswith(" " + k)):
                    resp = tr.get("response", "")
                    if isinstance(resp, list):
                        return random.choice(resp)
                    return random.choice(resp.splitlines())
    return None

# -----------------------
#  Main reply pipeline
# -----------------------
def get_smart_reply(user_text):
    # 1) try exact json responses first
    fast = find_in_json_exact(user_text)
    if fast:
        return fast

    # 2) ensure embeddings ready
    ensure_embeddings(force=False)

    # 3) semantic search
    sims = get_semantic_context(user_text, top_k=TOP_K)
    if not sims:
        return "B·∫°n ƒëang h·ªèi v·ªÅ v·∫•n ƒë·ªÅ n√†o v·∫≠y? Cho m√¨nh bi·∫øt d·ªãch v·ª• c·ª• th·ªÉ ƒë·ªÉ h·ªó tr·ª£ nh√©."

    # 4) check top score vs threshold
    top_score = sims[0]["score"]
    if top_score < SIMILARITY_THRESHOLD:
        # don't call OpenAI: ask clarifying question
        return "M√¨nh ch∆∞a th·∫•y th√¥ng tin r√µ r√†ng ‚Äî b·∫°n ƒëang h·ªèi v·ªÅ d·ªãch v·ª• n√†o trong s·ªë d·ªãch v·ª• c·ªßa b√™n m√¨nh? (v√≠ d·ª•: chatbot / thi·∫øt k·∫ø / ·ªëc) "

    # 5) filter to items belonging to dominant file to avoid cross-file mix
    files = [it["item"]["file"] for it in sims]
    dominant = max(set(files), key=lambda x: files.count(x))
    filtered = [s for s in sims if s["item"]["file"] == dominant]
    # if filtered empty fallback to sims
    top_items = filtered if filtered else sims

    # 6) assemble strict system prompt with only these top_items
    system_prompt = assemble_system_prompt(user_text, top_items)
    try:
        answer = call_openai_chat(system_prompt, user_text)
        # final safety: if answer contains phrases outside context? (simple guard)
        # if answer too generic or says "I don't know" -> ask user to clarify
        low_conf_phrases = ["i don't know", "i'm not sure", "kh√¥ng c√≥ th√¥ng tin", "m√¨nh ch∆∞a bi·∫øt"]
        if any(p in answer.lower() for p in low_conf_phrases):
            return "M√¨nh ch∆∞a c√≥ th√¥ng tin c·ª• th·ªÉ, b·∫°n cho m√¨nh bi·∫øt d·ªãch v·ª• ho·∫∑c chi ti·∫øt h∆°n ƒë∆∞·ª£c kh√¥ng?"
        return answer
    except Exception as e:
        print("‚ùå OpenAI chat error:", e)
        return "H·ªá th·ªëng AI ƒëang b·∫≠n, b·∫°n th·ª≠ l·∫°i sau 1 ph√∫t nh√©."

# -----------------------
#  Facebook send helper
# -----------------------
def send_text(psid, text):
    if not psid or not text:
        return
    try:
        requests.post(FB_SEND_URL, json={
            "recipient": {"id": psid},
            "message": {"text": text}
        }, timeout=15)
    except Exception as e:
        print("‚ùå FB send error:", e)

# -----------------------
#  Webhook
# -----------------------
@app.route("/webhook", methods=["GET"])
def verify():
    if request.args.get("hub.verify_token") == VERIFY_TOKEN:
        return request.args.get("hub.challenge")
    return "Sai verify token", 403

@app.route("/webhook", methods=["POST"])
def webhook():
    data = request.get_json(force=True, silent=True) or {}
    for entry in data.get("entry", []):
        for evt in entry.get("messaging", []):
            if evt.get("message", {}).get("is_echo"):
                continue
            psid = evt.get("sender", {}).get("id")
            text = evt.get("message", {}).get("text")
            if psid and text:
                print(f"üë§ {psid} -> {text}")
                reply = get_smart_reply(text)
                print("ü§ñ reply:", reply)
                send_text(psid, reply)
    return "OK", 200

@app.route("/health")
def health():
    return jsonify(
        ok=True,
        num_files=len(DATABASE),
        files=list(DATABASE.keys()),
        embed_count=len(EMBED_STORE["vectors"]) if EMBED_STORE else 0
    )
# ======================================================
#  ENDPOINT REBUILD EMBEDDINGS (t·ª± ƒë·ªông x√≥a + build l·∫°i)
# ======================================================
@app.route("/rebuild-embed", methods=["GET"])
def rebuild_embed():
    try:
        # Xo√° file embeddings_store.json n·∫øu t·ªìn t·∫°i
        if os.path.exists("embeddings_store.json"):
            os.remove("embeddings_store.json")
            msg = "ƒê√£ x√≥a embeddings_store.json. B·∫Øt ƒë·∫ßu build l·∫°i..."
            print("‚ö†Ô∏è", msg)
        else:
            msg = "Kh√¥ng th·∫•y embeddings_store.json. S·∫Ω build m·ªõi."

        # Build l·∫°i (force = True)
        ensure_embeddings(force=True)

        return {
            "ok": True,
            "message": "Rebuild embeddings th√†nh c√¥ng.",
            "detail": msg
        }

    except Exception as e:
        print("‚ùå L·ªói rebuild embeddings:", e)
        return {"ok": False, "error": str(e)}, 500

if __name__ == "__main__":
    # force-build embeddings once before serving (optional)
    # ensure_embeddings(force=True)
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 8080)))


